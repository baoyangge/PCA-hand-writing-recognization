{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS4_PCA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l96XD2NrEUip"
      },
      "source": [
        "# 基盤データサイエンス演習 第4回\n",
        "\n",
        "※本演習資料の二次配布・再配布はお断り致します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j5CT7A4rqTe"
      },
      "source": [
        "　今回の演習の内容は以下の通りである。\n",
        "\n",
        "　**DS4.0 | digitsデータセット**\n",
        "\n",
        "　**DS4.1 | 主成分分析(principal component analysis: PCA)のdigitsデータセットへの適用**\n",
        "\n",
        "　**DS4.2 | 主成分からの再構築**\n",
        "\n",
        "　**DS4.3 | 主成分分析における分散共分散行列の固有値**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv_jpkJu7PPe"
      },
      "source": [
        "## DS4.0 | digitsデータセット\n",
        "\n",
        "　今回は、手書き文字認識のデータ digits を用いる。digits は手書きの数字（0〜9）1文字の白黒画像と、対応する正しい数字からなるデータセットである。\n",
        "\n",
        "　今回は scikit-learn 内に存在するデータをimportして使用するので、ファイルをアップロードする必要はない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIGknYHNCamq"
      },
      "source": [
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "d = datasets.load_digits()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJPb_-4JSuRr"
      },
      "source": [
        "まずは、どのような情報が含まれているデータなのか見てみよう。\n",
        "\n",
        "`dir()`関数は、引数が持っているデータの一覧を取得することができる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kiQ0n2KQGAz"
      },
      "source": [
        "print(dir(d))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2lIFyssT6GX"
      },
      "source": [
        "それぞれ、`DESCR`はデータセットの説明（description）、`data`はベクトル形式の画像データ、`images`は画像形式の画像データ、`target`は画像に対応する0〜9の数字、target_namesはtargetの一覧(`[0, 1, ..., 9]`)である。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpuLtoUUYoVg"
      },
      "source": [
        "画像に相当する入力データ`d.data`は、各行が画像1枚に相当し、各列に画像の各ピクセルに対応する値を並べた行列である。今回の入力画像は8x8の解像度なので、列数は64である。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "09Rwsmi2Cqn3"
      },
      "source": [
        "data = d.data\n",
        "print(data.shape[1]) # show the number of columns of data\n",
        "print(data) # show data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RexhnCu_Y0Um"
      },
      "source": [
        "画像に相当するデータ`images`はarrayのリストのような形式で保存されている。\n",
        "\n",
        "さて、画像と数字を対応させて表示してみよう。下のセルで、変数`image_index`で指定する番号の画像を表示できる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVGhVhFfvJ0M"
      },
      "source": [
        "# display digits image\n",
        "# change image_index and see each image with its label\n",
        "\n",
        "images = d.images # images (\"list of array\"-like)\n",
        "target = d.target # corresponding label numbers\n",
        "\n",
        "image_index = 72\n",
        "plt.figure(1, figsize=(3, 3))\n",
        "plt.imshow(images[image_index], cmap=plt.cm.gray_r)\n",
        "plt.title(f'An image of \"{target[image_index]}\"')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EddDx74ZbMsq"
      },
      "source": [
        "さて、これら0〜9の数字について、\n",
        "* どの数字とどの数字は互いに似ていて、どの数字とどの数字は互いに似ていないだろうか。\n",
        "* これらの数字が64次元の空間上にどのように分布しているかを簡単に知ることはできるだろうか。\n",
        "* 8x8の画像のうち、データを特徴付けるのはどの部分だろうか。\n",
        "\n",
        "これらのことを知ることができる手法の一つが主成分分析である。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqA-swxfD-v0"
      },
      "source": [
        "---\n",
        "##### 課題 DS4.1\n",
        "　digitsデータセットについて、\n",
        "1. 画像の総枚数\n",
        "2. 件数が最も少ない数字（最も少ないものが複数ある場合、そのうちの最も値が小さい数字）\n",
        "\n",
        "をそれぞれ調べよ。\n",
        "\n",
        "（ヒント：1000件以上あるので目視で調べることは推奨しない。 `for` 文を用いて効率よく調べること）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVOLpa18_e14"
      },
      "source": [
        "---\n",
        "## DS4.1 | 主成分分析 (principal component analysis: PCA) のdigitsデータセットへの適用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEtkNzGFCqn1"
      },
      "source": [
        "　PCAは可視化や次元削減などに用いられる教師なし学習手法の一つで、\n",
        "データセットのばらつきの情報を保ってデータを低次元の空間に射影することができる。\n",
        "まず、PCAを用いてデータを2次元に射影し、どの数字が空間上のどこに対応するか見てみよう。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDkoVGBxW5mH"
      },
      "source": [
        "# prepare PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2) # 2 components will be used\n",
        "pca.fit(data) # obtain principal component\n",
        "images_map = pca.transform(data) # transform input data using PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MtY_UfbqCqn8"
      },
      "source": [
        "# scatter plot of PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "n_labels = 10\n",
        "\n",
        "# set color palette for plot\n",
        "color = [plt.cm.nipy_spectral(i/n_labels, 1) for i in range(n_labels)] \n",
        "\n",
        "# plot by each label\n",
        "for i in range(n_labels):\n",
        "  plt.scatter(images_map[target==i, 0], images_map[target==i, 1], \n",
        "              color=color[i], marker=f\"${i}$\")\n",
        "\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KorpoMOZw8Z"
      },
      "source": [
        "正しく実行されていれば、最初の二つの主成分の値に沿って各数字が出力されているはずである。\n",
        "\n",
        "上の図からどのようなことが言えるだろうか。例えば、0や4は他の数字と比較的離れた分布をしていること、5, 8, 9が近い位置に分布していることなどが見て取れる。\n",
        "\n",
        "この例では、64次元のデータを2次元に落とし込むことによって、データの分布をある程度可視化することができている。一方、数字間の重なりが大きく、2次元だけでは画像の情報を十分表現できていないこともわかる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWTR24_y_Lby"
      },
      "source": [
        "　また、PCAでは、各主成分（固有ベクトル）の値を見ることで、それぞれの主成分が主にどの列から構成されているか確認することができる。\n",
        "固有ベクトルに対応する固有値の正の平方根を掛けて正規化したものが主成分負荷（principal component loading）である。\n",
        "\n",
        "上位3つの主成分について、各主成分の内訳を見てみよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryaHtUOq_4QK"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pca = PCA(n_components=3).fit(data) # PCA, use 3 components\n",
        "components = pca.components_ # get components from PCA result\n",
        "\n",
        "indexes = [\"PC1\", \"PC2\", \"PC3\"]\n",
        "print(pd.DataFrame(components, index=indexes)) # print shaped result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyKumyCACqoJ"
      },
      "source": [
        "　これでは全体の様子がよくわからないので、固有ベクトルを画像と同じ形式に直し、値の大小を色で表して可視化してみよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "c3SfyBJYCqoK"
      },
      "source": [
        "# show components on image\n",
        "\n",
        "import numpy as np\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "for i in range(3): # for each component:\n",
        "  \n",
        "  plt.subplot(1, 3, i+1) # 3つに分割して図を描画する。i=0の時は一番左(i+1 = 1番目)。\n",
        "  components_reshape = np.reshape(components[i], images[0].shape) # 1D vector -> 2D image-like format\n",
        "\n",
        "  im = plt.imshow(components_reshape, cmap=plt.cm.jet)\n",
        "  im.set_clim(-0.3,0.3) # set color range\n",
        "  plt.title(\"principal component of PC\"+str(i+1))\n",
        "\n",
        "  plt.colorbar()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCGwI23vNGuG"
      },
      "source": [
        "　ここで、固有ベクトルの各値の**符号の正負ではなく**、値の**絶対値**が大きい説明変数がデータの分散をより表現しているといえることに注意する必要がある。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vML4KYI5B8se"
      },
      "source": [
        "　各主成分の寄与率 （explained variance ratio） は、元のデータの分散（$=$分散共分散行列の固有値の合計）のうちどれだけの割合をその主成分（に対応する固有値）が持っているかに相当する。実際に上位5つの主成分を用いて寄与率を見てみよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed7sgUqaBw_o"
      },
      "source": [
        "n_components = 5\n",
        "indexes = [f\"PC{i+1}\" for i in range(n_components)]\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "\n",
        "pca = PCA(n_components=n_components).fit(data) # PCA, use 5 components\n",
        "explained_variance = pca.explained_variance_ratio_ # explained ratio\n",
        "plt.bar(np.arange(n_components), explained_variance,\n",
        "        tick_label=indexes)\n",
        "plt.ylabel(\"explained variance ratio\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyJTCzLMzgm8"
      },
      "source": [
        "同様に、累積の寄与率は以下のようになる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU-mprWmvXyO"
      },
      "source": [
        "fig = plt.figure(figsize=(6,4))\n",
        "cumulative_explained_variance = np.cumsum(explained_variance)\n",
        "\n",
        "plt.plot(np.arange(n_components), cumulative_explained_variance, marker=\"o\")\n",
        "plt.xticks(range(n_components), indexes)\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel(\"cumulative explained variance ratio\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0b_yoWqvBNn"
      },
      "source": [
        "PCAでは固有値を大きい順に用いるので、寄与率は徐々に減少していく。\n",
        "\n",
        "すべての主成分の寄与率の合計は1になるので、累積の寄与率をいくつの主成分を用いるかの基準として用いることができる。この場合、累積の寄与率の目安として0.7や0.8が用いられることがある。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOKxDUXQDByq"
      },
      "source": [
        "---\n",
        "##### 課題 DS4.2\n",
        "　上のPCAの結果から、digitsデータセットの画像では、画像のどのような領域がデータの分散を表している（データによって大きく異なる）といえるか、考察せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1yMkMOu0tke"
      },
      "source": [
        "---\n",
        "##### 課題 DS4.3\n",
        "　PCAを行う際には、中心化と言って、まず各変数の平均が0になるように前処理するのが一般的である（※1）。例えば今回の画像のケースの場合、一番左上の画素 (pixel) の値について、データセット内での平均を0にするような操作である。\n",
        "\n",
        "　scikit-learnの `PCA()` では中心化を自動でやってくれているのだが、その具体的な処理について知っておこう。下のコードは入力されたデータセット（データの集合）に対して中心化を行う関数 `centering(X)` である。これを用いて `d.data` を中心化せよ。\n",
        "ただし、レポートには中心化後の最初の画像 `data[0, :]` の値のうち最も大きい値を、小数第三位を四捨五入して答えること。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvh3JZxV7EYj"
      },
      "source": [
        "def centering(X):\n",
        "  return X - np.mean(X, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-3GCaHa7mB5"
      },
      "source": [
        "----\n",
        "\n",
        "## DS4.2 | 主成分からの再構築\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMss61Xn8wxo"
      },
      "source": [
        "　PCAは次元削減にも利用される手法であった。PCAを行った後、いくつかの主成分だけを残すことによって、元の行列をより小さい次元で近似することができる。このとき、元のデータの情報はどの程度残っているのだろうか。これはPCAで次元削減を行い、その結果に逆変換を行ってデータを元の形式に戻すことによって視覚的に確認することができる。\n",
        "\n",
        "　下のセル群を`n_components`を変えながら何度か実行することで、使用した次元数と復元後の関係を見ることができる。復元の精度が良いほど、より多くの情報が残っているといえる。\n",
        "\n",
        "　下のセル群のうち、上のセルが変換・逆変換を行う関数で、下の2セルがプロットする部分である。`n_components`で削減する次元数を、`image_index`で表示する画像を指定することができる。元の画像が64次元であるため、`n_components`には1以上64以下の値を指定すること。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGamiVbc9Bnk"
      },
      "source": [
        "def project_reconstruct(data, n_components):\n",
        "  pca = PCA(n_components=n_components)\n",
        "  pca.fit(data)\n",
        "  data_map = pca.transform(data) # projection\n",
        "  data_reconstructed = pca.inverse_transform(data_map) # reconstruction\n",
        "  sum_explained_variance_ratio = np.sum(pca.explained_variance_ratio_)\n",
        "\n",
        "  return data_reconstructed, sum_explained_variance_ratio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb2bfpl9A4aC"
      },
      "source": [
        "data = datasets.load_digits().data\n",
        "# project and reconstruct of data by PCA\n",
        "n_components = 3 # the number of components to be used\n",
        "\n",
        "data_reconstructed, sum_explained_variance_ratio = \\\n",
        "  project_reconstruct(data, n_components)\n",
        "print(f\"Cumulative explained variance ratio: {sum_explained_variance_ratio:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPxbxTILzhzn"
      },
      "source": [
        "# show above result\n",
        "image_index = 11 # the index of image to be shown\n",
        "image_reconstructed = np.reshape(data_reconstructed[image_index],\n",
        "                                 images[0].shape)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "# show original image\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(images[image_index], cmap=plt.cm.gray_r)\n",
        "plt.title(f'Original image. Label: \"{target[image_index]}\"')\n",
        "\n",
        "# show reconstructed image\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(image_reconstructed, cmap=plt.cm.gray_r)\n",
        "plt.title(f\"Reconstructed from {n_components} principal components\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6exKKXZ4J7I"
      },
      "source": [
        "実行した結果から、残した次元数が多いほど復元の精度が良い、つまり情報が多く残っていることが見て取れるはずである。\n",
        "\n",
        "このように、元々のデータが視覚的に捉えられる場合には、実際に復元してみることでも使用するべき主成分の数を判断することができる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYZO0ToXCqoe"
      },
      "source": [
        "---\n",
        "## DS4.3 | 主成分分析における分散共分散行列の固有値"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3f-c90ECqog"
      },
      "source": [
        "　講義ですでに証明したように、PCAで用いる主成分およびその寄与は元のデータの分散共分散行列（単に共分散行列とも言う）の固有ベクトルおよび固有値から求めることができる。`PCA.fit`では「分散共分散行列の固有ベクトルおよび固有値を計算し、大きい順に並べ替えて答える」ことに相当する処理（※2）を行なっている。\n",
        "\n",
        "　ここではこの計算を**scikit-learnを通さず**行い、scikit-learnのPCAと同様の結果が得られることを確認する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFJt7lRi5ucB"
      },
      "source": [
        "# 課題 DS4.3 と同一の関数\n",
        "def centering(X):\n",
        "  return X - np.mean(X, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjndoiFWCqoi"
      },
      "source": [
        "# calculate covariance matrix\n",
        "data_centered = centering(data)\n",
        "cov_matrix = np.cov(data_centered, rowvar=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRrdUdDzCqol"
      },
      "source": [
        "# calculate eigenvalue/eigenvector of covariance matrix\n",
        "from numpy.linalg import eig\n",
        "eig_val, eig_vec = eig(cov_matrix)\n",
        "\n",
        "# sort by eigenvector (descending)\n",
        "n_components = 3\n",
        "eig_index = np.argsort(eig_val)      # argsortは要素の昇順になるように添え字を並べる\n",
        "eig_index = eig_index[::-1]          # 降順を得るために[::-1]で順番を逆にする\n",
        "eig_index = eig_index[:n_components] # 降順でn_components個だけ添え字を取得する\n",
        "eig_val = eig_val[eig_index]         # 固有値を大きい順に取得\n",
        "eig_vec = eig_vec.T[eig_index]       # 対応する固有ベクトルを取得"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW33ZU2wCqoo"
      },
      "source": [
        "# show\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "for i in range(n_components):\n",
        "  \n",
        "  plt.subplot(1, n_components, i+1)\n",
        "  eig_vec_reshape = np.reshape(eig_vec[i], images[0].shape)\n",
        "\n",
        "  im = plt.imshow(eig_vec_reshape, cmap=plt.cm.jet)\n",
        "  im.set_clim(-0.3,0.3)\n",
        "  plt.title(\"principal component of PC\"+str(i+1))\n",
        "  plt.colorbar()\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auvsC_s-C2YU"
      },
      "source": [
        "　主成分（固有ベクトル）を見ることにより、scikit-learnのPCAと同様の結果になっていることが確認できる。\n",
        "なお、主成分の符号が反転している場合があるが、ベクトルの向きが反対になっているだけで表している意味は同じである。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNo0r7QRG_On"
      },
      "source": [
        "---\n",
        "##### 課題 DS4.4\n",
        "　固有ベクトルが等しいことを確認したので、固有値も等しくなっていることを確認しよう。\n",
        "PCA関数で計算した固有値と直接計算した固有値が一致することを確認し、最も大きい固有値を小数点以下を四捨五入し整数で答えよ。\n",
        "ただし、PCA関数で計算した結果の固有値は`pca.explained_variance_`で取得することができる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFWDiOzmqP1I"
      },
      "source": [
        "----\n",
        "# レポート提出について\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8RG8GPuqjs4"
      },
      "source": [
        "## レポートの提出方法\n",
        "\n",
        "　レポートは**答案テンプレートを用い**、**1つのファイル（.doc, .docx, .pdf）**にまとめ、**学籍番号と氏名を確認の上**、**次回 基盤データサイエンス演習 の開始時刻までに東工大ポータルのOCW-iから提出**すること。\n",
        "ファイルのアップロード後、OCW-iで「提出済」というアイコンが表示されていることを必ず確認すること。それ以外の場合は未提出扱いとなるので十分注意すること。\n",
        "また、締め切りを過ぎるとファイルの提出ができないため、時間に余裕を持って提出を行うこと。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg3jzH6XDQnN"
      },
      "source": [
        "### 答案テンプレート\n",
        "```\n",
        "学籍番号:\n",
        "\n",
        "名前:\n",
        "\n",
        "課題 DS4.1\n",
        "\n",
        "画像の総枚数：\n",
        "件数が最も少ない数字：\n",
        "\n",
        "課題 DS4.2\n",
        "\n",
        "(課題 DS4.2の答案をここに記入すること)\n",
        "\n",
        "課題 DS4.3\n",
        "\n",
        "中心化後の最初の画像の値のうち最も大きい値: __________\n",
        "\n",
        "課題 DS4.4\n",
        "\n",
        "最も大きい固有値: __________\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvVtUbX19JvF"
      },
      "source": [
        "# 補足資料"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tryKPIvno9JH"
      },
      "source": [
        "### ※1 PCAの中心化について\n",
        "\n",
        "　PCAでは中心化が行われるが、正規化（平均を 0、分散を 1 にする）ではないのか？と思われる人もいるだろう。これは良い発想で、分散を 1 にする方が良い場合と良くない場合がある。[scikit-learnのPCAのドキュメント](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)によれば、（データの大きさを揃えることで）変換されたデータの分散の大きさの情報が失われるが、機械学習の入力として用いる場合は予測精度が向上する可能性がある。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0a8groR1EgQ"
      },
      "source": [
        "※2 実際のPCA.fitで行われている処理\n",
        "\n",
        "　実際には、`PCA.fit()`では分散共分散行列の固有値・固有ベクトルを求める処理の代わりに、元の行列の特異値分解の結果から分散共分散行列の固有値・固有ベクトルを求めている。 詳細は[実装](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/_pca.py)などを参照。"
      ]
    }
  ]
}